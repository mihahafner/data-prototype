Below are **two perfectly structured Markdown documents**, one for **Week 1** and one for **Week 2**, ready to copy-paste into:

```
/docs/week1_summary.md
/docs/week2_summary.md
```

Both are written as **proper project documentation** â€” clean, precise, and fully aligned with what you built.

---

# âœ… **WEEK 1 SUMMARY â€” `docs/week1_summary.md`**

```md
# Week 1 â€” Project Setup, API Fetch, Reproducible Pipeline

## ğŸ¯ Objective
Establish a fully reproducible data pipeline with:
- a clean project structure,
- GitHub versioning,
- weather data ingestion via API,
- reproducible execution in both **local PyCharm** and **Google Colab**.

This week defines the *foundation* for all future ETL, EDA, ML and reporting steps.

---

# 1. Project Structure

```

data_prototype/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 â† DO NOT COMMIT (gitignored)
â”‚   â”œâ”€â”€ processed/           â† DO NOT COMMIT (gitignored)
â”‚   â””â”€â”€ proto.db             â† DO NOT COMMIT (gitignored)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_api_to_csv.ipynb
â”‚   â”œâ”€â”€ 02_sql_eda.ipynb     (created Week 2)
â”‚   â””â”€â”€ 03_anomaly_report.ipynb (created Week 2)
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/             â† Generated plots (Week 2)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ week1_summary.md
â”‚   â””â”€â”€ week2_summary.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ verify_environment.py

```

**Why this structure?**
- Separates *source code* from *data outputs*.
- Keeps GitHub clean.
- Supports Colab execution and local execution identically.

---

# 2. GitHub Setup

### Steps performed:
- Created repo: **mihahafner/data-prototype**
- Cloned locally via PyCharm.
- Configured global git identity.
- Added `.gitignore`:
```

data/raw/
data/processed/
data/*.db

````
- Pushed initial commit with:
- folder structure
- README.md
- requirements.txt
- verify_environment.py

---

# 3. Notebook: Fetch Weather API â†’ Save CSV
**File:** `notebooks/01_api_to_csv.ipynb`

### Key features:
### âœ” Universal path resolver (works on local + Colab)

```python
REPO_URL = "https://github.com/mihahafner/data-prototype.git"
def get_repo_root():
  in_colab = "google.colab" in sys.modules
  ...
REPO_ROOT = get_repo_root()
RAW = REPO_ROOT / "data" / "raw"
````

### âœ” Weather API call (Open-Meteo)

* Fetch temperature, humidity, timestamps.
* Convert JSON â†’ pandas DataFrame.
* Save â€œatomic writeâ€ CSV:

  ```
  data/raw/weather_lju.csv
  ```

### âœ” Kernel restart issues fixed

* Cleaned state
* Ensured CSV writes outside notebook working directory

---

# 4. Running in Google Colab

### Implemented:

* Auto-clone repo when notebook runs in Colab
* Automatically set paths so CSV files save correctly
* Successfully executed all cells
* Verified reproducibility between:

  * Windows PyCharm
  * Google Colab environment

---

# 5. Final Deliverables (Week 1)

* `notebooks/01_api_to_csv.ipynb`
* `README.md` with project description
* Reproducible pipeline that downloads weather data and writes CSV
* Local + Colab execution fully functional

---

# âœ” Week 1 Completed Successfully

This concludes Week 1: **You now have a fully reproducible, versioned, API-driven data ingestion pipeline.**
