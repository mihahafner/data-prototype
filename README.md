
data_prototype/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/weather_lju_anomalies.csv
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ s3_publish.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                â† week summary
â””â”€â”€ README_s3_publish.md     â† tool-specific readme




# âœ… **WEEK 1 â€” PROJECT FOUNDATION & REPRODUCIBLE DATA INGESTION PIPELINE**

This document summarizes everything implemented in **Week 1** of the *data-prototype* project.
It describes the **architecture**, **components**, **data flow**, **processes**, and how the entire workflow is **fully reproducible** both:

* **locally (PyCharm)**
* **in the cloud (Google Colab)**

This README serves as a **design plan**, a **technical reference**, and a **runbook** for anyone joining the project.

---

# ğŸš€ **1. PROJECT GOALS FOR WEEK 1**

Week 1 establishes the complete infrastructure required for a growing data project:

### âœ” Fully working local Python environment

### âœ” Fully working cloud execution (Google Colab)

### âœ” Reproducible folder structure

### âœ” Automated data ingestion from Open-Meteo API

### âœ” Clean Git history + `.gitignore`

### âœ” Data stored locally but *never* committed to Git

### âœ” Notebook working in *both* environments (local + Colab)

### âœ” First dataset exported to `data/raw/weather_lju.csv`

These steps create a robust foundation for Week 2 (processing + features).

---

# ğŸ“‚ **2. PROJECT STRUCTURE (REPRODUCIBLE ACROSS SYSTEMS)**

Only source code, configs, and notebooks are stored in Git.
Data is generated automatically.

```
data-prototype/
â”‚
â”œâ”€â”€ data/                     â† Local only (gitignored)
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ .gitkeep          â† Keeps folder structure in Git
â”‚   â””â”€â”€ processed/            â† Future weeks
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_api_to_csv.ipynb   â† Week 1 notebook (runs local + Colab)
â”‚
â”œâ”€â”€ verify_environment.py     â† Ensures correct Python environment
â”œâ”€â”€ requirements.txt          â† Reproducible dependencies
â”œâ”€â”€ README.md                 â† This document
â””â”€â”€ .gitignore                â† Prevents data & temp files from entering Git
```

### âœ” Clean

### âœ” Standard ML/DS layout

### âœ” Data folders exist **without** containing data

### âœ” Git contains only what matters (code, configs, notebooks)

---

# ğŸŒ **3. ENVIRONMENTS**

Week 1 ensures the project runs identically in two execution environments:

---

## ğŸ–¥ï¸ **Local: PyCharm (Windows)**

* Python 3.14 venv created inside the project
* Dependencies installed from `requirements.txt`
* Notebook executed through PyCharm Jupyter integration
* CSV stored locally in:
  `data/raw/weather_lju.csv`

---

## â˜ï¸ **Cloud: Google Colab**

Colab is used for team collaboration and remote execution.

When executed in Colab:

1. The notebook detects it is running in Colab.
2. It clones the repository automatically:

```
/content/data-prototype/
```

3. It sets the working directory to the repo root.
4. It runs the same code and writes data to:

```
/content/data-prototype/data/raw/weather_lju.csv
```

### âœ” Same folder structure

### âœ” Same code

### âœ” Same results

### âœ” No manual setup required

---

# ğŸ”§ **4. UNIVERSAL PATH MANAGEMENT**

*(the core of Week 1 engineering work)*

To support both environments, the notebook uses a **universal project root resolver**:

### âœ” Works locally

### âœ” Works in Colab

### âœ” Automatically clones the repo in Colab

### âœ” Always finds the repo root

### âœ” Always ensures `data/raw/` exists

```python
from pathlib import Path
import os, sys, subprocess

REPO_URL = "https://github.com/mihahafner/data-prototype.git"
REPO_DIRNAME = "data-prototype"

def get_repo_root() -> Path:
    in_colab = "google.colab" in sys.modules

    if in_colab:
        root = Path("/content") / REPO_DIRNAME
        if not root.exists():
            subprocess.run(
                ["git", "clone", "--depth", "1", REPO_URL, str(root)],
                check=True
            )
        os.chdir(root)
        return root

    here = Path.cwd()
    for p in (here, *here.parents):
        if (p / ".git").is_dir():
            return p

    for p in (here, *here.parents):
        if (p / "data").is_dir():
            return p

    return here

REPO_ROOT = get_repo_root()
RAW = REPO_ROOT / "data" / "raw"
RAW.mkdir(parents=True, exist_ok=True)
```

---

# ğŸŒ¦ï¸ **5. DATA INGESTION PIPELINE (OPEN-METEO API)**

The notebook performs the following:

## 1ï¸âƒ£ Fetch hourly temperature + humidity for Ljubljana

via Open-Meteo free API.

```python
import requests
response = requests.get("https://api.open-meteo.com/...")
json_data = response.json()
```

## 2ï¸âƒ£ Convert to DataFrame

```python
import pandas as pd
df = pd.DataFrame({
    "time": json_data["hourly"]["time"],
    "temp_c": json_data["hourly"]["temperature_2m"],
    "rh": json_data["hourly"]["relative_humidity_2m"],
})
```

## 3ï¸âƒ£ Save CSV locally (atomic write)

```python
tmp = RAW / "weather_lju.csv.tmp"
out = RAW / "weather_lju.csv"

df.to_csv(tmp, index=False, encoding="utf-8")
tmp.replace(out)   # atomic replacement
```

## 4ï¸âƒ£ Verify it

### Local:

```
data/raw/weather_lju.csv
```

### Colab:

```
/content/data-prototype/data/raw/weather_lju.csv
```

---

# ğŸš« **6. GIT STRATEGY â€” DATA SHOULD NEVER GO INTO GIT**

`.gitignore` contains:

```
data/
!data/raw/.gitkeep
```

Meaning:

* All data files are ignored âŒ
* Folder structure stays in Git âœ”
* No CSV uploaded
* No large data history
* Clean project

This is critical for scaling into Weeks 2â€“8.

---

# ğŸ” **7. REPRODUCIBILITY**

Anyone can clone the repo and reproduce Week 1:

### ğŸ‘‰ Local:

```bash
git clone https://github.com/mihahafner/data-prototype.git
cd data-prototype
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
pycharm .
```

Run:
`notebooks/01_api_to_csv.ipynb`
Click *Run All* â†’ CSV generated.

---

### ğŸ‘‰ Colab:

Open:

```
https://colab.research.google.com/github/mihahafner/data-prototype/blob/main/notebooks/01_api_to_csv.ipynb
```

Click:

**"Run Anyway" â†’ Run All**

Everything is reproduced automatically:

* Repo cloned âœ”
* Paths resolved âœ”
* Data fetched âœ”
* CSV saved âœ”

---

# ğŸ§± **8. WEEK 1 ARCHITECTURE OVERVIEW**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Repository      â”‚
â”‚ (code, notebooks, config)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â”‚ cloned (Colab) / pulled (local)
                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Execution Environment  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â€¢ Local (PyCharm)    â”‚             â”‚
  â”‚   â€¢ Colab (Cloud)      â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
              â”‚                          â”‚
   resolve repo root path                â”‚
              â–¼                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚  data/ directory   â”‚   â† ignored by Git
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
              â”‚                          â”‚
              â–¼                          â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
     â”‚ Open-Meteo API     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ¯ **9. WEEK 1: DELIVERABLES COMPLETED**

### âœ” Local + Colab unified execution

### âœ” Automatic repo cloning in Colab

### âœ” API ingestion

### âœ” Clean & structured data folder

### âœ” Correct .gitignore

### âœ” First data export

### âœ” Professional folder architecture

### âœ” No data committed to Git

### âœ” README and requirements ready

Your project now has a **solid engineering foundation**.

---

# ğŸ§­ **10. NEXT STEPS (WEEK 2 PREVIEW)**

Week 2 will focus on:

* reading raw CSV
* cleaning & preprocessing
* handling missing values
* making processed datasets
* writing data to `data/processed/`
* building reusable Python modules
* versioning processed datasets
* adding unit tests

---

If you'd like, I can generate:

### ğŸ”¹ A diagram

### ğŸ”¹ A PDF version of Week 1 summary

### ğŸ”¹ A project roadmap for all 8 weeks

### ğŸ”¹ A â€œNext Stepsâ€ notebook for Week 2

Just tell me.
